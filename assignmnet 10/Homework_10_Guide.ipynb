{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-homework",
   "metadata": {},
   "source": [
    "### Homework 10 Guide\n",
    "<br>\n",
    "In this guide, we will be applying LDA. You will need to download the amazon review dataset linked from the canvas assignment for this guide. For the last time, let's get our libraries imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-feedback",
   "metadata": {},
   "source": [
    "Like last week, we need to convert the data over to a pandas dataframe from the current format. Run the cell below to get your dataframe ready. You'll need to write the path to your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = # Your Code Here\n",
    "f = open(path, \"r\")\n",
    "\n",
    "data =[]\n",
    "# Converting it to pandas dataframe\n",
    "for line in f:\n",
    "    review = line[:len(line) - 2]\n",
    "    sentiment = \"neg\" if line[len(line)-2] == \"0\" else \"pos\"\n",
    "    row = [review, sentiment]\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['reviews', 'sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-share",
   "metadata": {},
   "source": [
    "With our dataframe made, we now need to clean it before analyzing. Apply the `remove_punctuation()` and `remove_stopwords()` functions on our dataset to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the line below if you need to download the stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df['reviews'] = # Your Code Here\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-aviation",
   "metadata": {},
   "source": [
    "We need to adjust our data slightly before using LDA. In the cell below, use the `CountVectorizer()` function. Then, use `fit_transform()` with `df['reviews']` as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = _(max_features = 5000, max_df=.15) # Your Code Here\n",
    "X = vect._(_) # Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-gross",
   "metadata": {},
   "source": [
    "Using the `LatenDirichletAllocation()` function below, we want to pass it 10 components. You can adjust the max iterations for your local setup, or leave it as 25 if unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = _(n_components=_, learning_method=\"batch\", max_iter=25, random_state=0) # Your Code Here\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-answer",
   "metadata": {},
   "source": [
    "And finally' let's see the results! Call the `print_topics()` function below, passing in `feature_names` and `sorting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "#slide 27\n",
    "def print_topics(topics, feature_names, sorting, topics_per_chunk, n_words):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        len_this_chunk = len(topics)\n",
    "        \n",
    "        print(*these_topics)\n",
    "        print(\"----------------------\")\n",
    "\n",
    "\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print(*feature_names[sorting[these_topics, i]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "            \n",
    "_(topics=range(10), feature_names=_, sorting=_, topics_per_chunk=5, n_words=10) # Your Code Here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-insertion",
   "metadata": {},
   "source": [
    "And thats the final guide! Save your version with the outputs intact and submit on canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
